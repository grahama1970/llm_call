# This is the master plan for the Claude Code Orchestrator.
# It is declarative, readable, and defines the sequence of operations.
# Version 2: All issues from the critique have been addressed.

# The root directory for the entire operation. All subsequent paths are relative to this.
working_directory: "/home/graham/workspace/experiments/llm_call/src/llm_call/usage/docs/tasks/claude_poll_poc_v2/"

settings:
  log_file: "task_execution.log"

tasks:
  - task_id: T1_ENV_SETUP
    description: "Archive old artifacts and create a fresh environment."
    action:
      type: llm_generate_and_run
      prompt_template: "commands/archive-setup.md"
      output_path: "generated_scripts/setup.py"
      params:
        log_file: "{{ settings.log_file }}"
    verify:
      - type: file_exists
        path: "generated_scripts/"
      - type: log_contains
        log_file: "{{ settings.log_file }}"
        pattern: "Created fresh generated_scripts/ directory"

  - task_id: T2_CREATE_CODE
    description: "Create a simple Python script for testing."
    action:
      type: write_file
      path: "generated_scripts/simple_add.py"
      content: |
        # A simple function to add two numbers and write to a file.
        def add_numbers():
          result = 2 + 3 
          with open("add_results.txt", "w") as f:
            f.write(f"The sum is: {result}")
        if __name__ == "__main__":
          add_numbers()
    skip_if:
      - type: file_exists
        path: "generated_scripts/simple_add.py"

  - task_id: T3_RUN_CODE_INLINE
    description: "Execute the simple script directly."
    action:
      type: execute_command
      command: "python generated_scripts/simple_add.py"
    verify:
      - type: file_contains
        path: "add_results.txt"
        content: "The sum is: 5"

  - task_id: T4_BACKGROUND_VERIFICATION
    description: "Launch a background Claude instance to critique the code and verify its output."
    action:
      type: llm_generate_and_run
      mode: background
      prompt_template: "commands/claude-verify.md"
      output_path: "generated_scripts/verify_script.py"
      params:
        code_file: "generated_scripts/simple_add.py"
        result_file: "add_results.txt"
        status_file: "verification_status.json"
        log_file: "{{ settings.log_file }}"
        expected_result: "The sum is: 5" # The exact string to search for

  - task_id: T5_POLL_FOR_VERIFICATION
    description: "Wait for the background verification to complete successfully."
    action:
      type: llm_generate_and_run
      prompt_template: "commands/claude-poll.md"
      output_path: "generated_scripts/poll_script.py"
      params:
        status_file: "verification_status.json"
        expected_status: "pass"
        timeout: 60
        log_file: "{{ settings.log_file }}"
    # The actual polling is done by running the script generated above
    execute_after_generate:
      type: execute_command
      command: "python generated_scripts/poll_script.py"

  - task_id: T6_FINAL_AUDIT
    description: "Use Gemini to perform a final, holistic audit of the entire run."
    action:
      type: llm_generate_and_run
      prompt_template: "commands/ask-gemini-flash.md"
      output_path: "gemini_validation_response.md"
      params:
        output_path: "gemini_validation_response.md" # Pass output path as a param
        query: |
          Please provide a detailed critique of the following automated task execution. 
          Analyze the logs and artifacts to determine if the proof-of-concept succeeded.
          Specifically, did the background verification and polling mechanism work correctly?

          --- EXECUTION LOG ---
          {{ file_content('task_execution.log') }}

          --- VERIFICATION STATUS JSON ---
          {{ file_content('verification_status.json') }}

          --- FINAL RESULT FILE ---
          {{ file_content('add_results.txt') }}