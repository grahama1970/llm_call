#!/usr/bin/env python3
"""
LLM CLI with Full Configuration Support and Auto-Generated Slash Commands

This unified CLI provides:
1. Simple commands for quick tasks (ask, chat, models)
2. Full litellm configuration support (validation, retry, etc.)
3. Config file support (JSON/YAML) with CLI overrides
4. Auto-generated Claude Code slash commands
5. MCP server integration

Example usage:
    # Simple usage
    uv run python -m llm_call.cli.main ask "What is Python?"
    
    # With full configuration
    uv run python -m llm_call.cli.main ask "Generate code" --model gpt-4 --validate code --temp 0.3
    
    # Using config file
    uv run python -m llm_call.cli.main call config.json --prompt "Override prompt"
    
    # Generate slash commands
    uv run python -m llm_call.cli.main generate-claude
"""

import typer
from pathlib import Path
from typing import Optional, List, Dict, Any
import json
import sys
import yaml
import inspect
from loguru import logger
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax

# Initialize the CLI app
app = typer.Typer(

# Add slash command and MCP generation
from .slash_mcp_mixin import add_slash_mcp_commands
add_slash_mcp_commands(app)

    name="llm-cli",
    help="Unified LLM CLI with full configuration support and auto-generated slash commands"
)

console = Console()

# Global state for chat sessions
chat_history: List[Dict[str, str]] = []

# ============================================
# CONFIGURATION UTILITIES
# ============================================

def load_config_file(path: Path) -> Dict[str, Any]:
    """Load configuration from JSON or YAML file."""
    if path.suffix == '.json':
        with open(path) as f:
            return json.load(f)
    elif path.suffix in ['.yaml', '.yml']:
        with open(path) as f:
            return yaml.safe_load(f)
    else:
        raise ValueError(f"Unsupported config format: {path.suffix}")


def build_llm_config(
    prompt: str,
    model: Optional[str] = None,
    validation: Optional[List[str]] = None,
    retry_max: Optional[int] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    system_prompt: Optional[str] = None,
    response_format: Optional[str] = None,
    config_file: Optional[Path] = None
) -> Dict[str, Any]:
    """Build LLM configuration from various sources."""
    # Start with base config
    config = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    # Load from config file if provided
    if config_file:
        file_config = load_config_file(config_file)
        config.update(file_config)
        # Messages from CLI override file
        if prompt:
            config["messages"] = [{"role": "user", "content": prompt}]
    
    # Apply CLI overrides
    if model:
        config["model"] = model
    if validation:
        config["validation"] = [{"type": v} for v in validation]
    if retry_max is not None:
        config["retry_config"] = {"max_attempts": retry_max}
    if temperature is not None:
        config["temperature"] = temperature
    if max_tokens is not None:
        config["max_tokens"] = max_tokens
    if system_prompt:
        # Prepend system message
        messages = config.get("messages", [])
        config["messages"] = [{"role": "system", "content": system_prompt}] + messages
    if response_format:
        config["response_format"] = {"type": response_format}
    
    # Default model if not specified
    if "model" not in config:
        config["model"] = "gpt-3.5-turbo"
    
    return config

# ============================================
# CORE LLM COMMANDS
# ============================================

@app.command()
def ask(
    prompt: str = typer.Argument(..., help="Question or prompt for the LLM"),
    model: Optional[str] = typer.Option(None, "--model", "-m", help="Model to use (e.g., gpt-4, claude-3)"),
    validate: Optional[List[str]] = typer.Option(None, "--validate", "-v", help="Validation strategies to apply"),
    temperature: Optional[float] = typer.Option(None, "--temp", "-t", help="Temperature for response generation"),
    max_tokens: Optional[int] = typer.Option(None, "--max-tokens", help="Maximum tokens in response"),
    system: Optional[str] = typer.Option(None, "--system", "-s", help="System prompt to prepend"),
    json_mode: bool = typer.Option(False, "--json", "-j", help="Request JSON formatted response"),
    retry: Optional[int] = typer.Option(None, "--retry", "-r", help="Maximum retry attempts"),
    show_config: bool = typer.Option(False, "--show-config", help="Display the configuration being used"),
    stream: bool = typer.Option(False, "--stream", help="Stream the response")
):
    """
    Ask a single question to an LLM with optional configuration.
    
    Examples:
    - Basic: ask "What is Python?"
    - With model: ask "Explain AI" --model gpt-4
    - With validation: ask "Write a function" --validate code
    - Full config: ask "Create API" --model gpt-4 --validate code --temp 0.3 --system "You are an expert"
    """
    import asyncio
    from llm_call.core.caller import make_llm_request
    
    try:
        # Build configuration
        config = build_llm_config(
            prompt=prompt,
            model=model,
            validation=validate,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system,
            response_format="json_object" if json_mode else None,
            retry_max=retry
        )
        
        if stream:
            config["stream"] = True
        
        if show_config:
            console.print(Panel(
                Syntax(json.dumps(config, indent=2), "json"),
                title="LLM Configuration",
                border_style="blue"
            ))
        
        # Make the request
        console.print(f"[bold blue]Using model:[/bold blue] {config.get('model', 'default')}")
        
        if validate:
            console.print(f"[bold yellow]Applying validation:[/bold yellow] {', '.join(validate)}")
        
        with console.status("[bold green]Thinking..."):
            response = asyncio.run(make_llm_request(config))
        
        # Display response
        if isinstance(response, dict) and "content" in response:
            content = response["content"]
        elif hasattr(response, 'choices'):
            content = response.choices[0].message.content
        else:
            content = str(response)
        
        if json_mode:
            try:
                parsed = json.loads(content)
                console.print(Panel(
                    Syntax(json.dumps(parsed, indent=2), "json"),
                    title="Response",
                    border_style="green"
                ))
            except:
                console.print(Panel(content, title="Response", border_style="green"))
        else:
            console.print(Panel(content, title="Response", border_style="green"))
            
    except Exception as e:
        logger.error(f"LLM request failed: {e}")
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        raise typer.Exit(1)

@app.command()
def chat(
    model: Optional[str] = typer.Option(None, "--model", "-m", help="Model to use"),
    system: Optional[str] = typer.Option(None, "--system", "-s", help="System prompt"),
    temperature: Optional[float] = typer.Option(None, "--temp", "-t", help="Temperature")
):
    """
    Start an interactive chat session with an LLM.
    
    Examples:
    - Basic: chat
    - With model: chat --model gpt-4
    - With system: chat --system "You are a helpful coding assistant"
    """
    import asyncio
    from llm_call.core.caller import make_llm_request
    
    global chat_history
    
    console.print("[bold green]Starting chat session. Type 'exit' or 'quit' to end.[/bold green]")
    
    if system:
        chat_history.append({"role": "system", "content": system})
        console.print(f"[dim]System: {system}[/dim]")
    
    selected_model = model or "gpt-3.5-turbo"
    console.print(f"[bold blue]Using model:[/bold blue] {selected_model}")
    
    while True:
        try:
            user_input = console.input("[bold cyan]You:[/bold cyan] ")
            
            if user_input.lower() in ["exit", "quit"]:
                console.print("[bold yellow]Ending chat session.[/bold yellow]")
                break
            
            chat_history.append({"role": "user", "content": user_input})
            
            config = {
                "model": selected_model,
                "messages": chat_history,
            }
            
            if temperature is not None:
                config["temperature"] = temperature
            
            with console.status("[bold green]Thinking..."):
                response = asyncio.run(make_llm_request(config))
            
            if isinstance(response, dict) and "content" in response:
                content = response["content"]
            elif hasattr(response, 'choices'):
                content = response.choices[0].message.content
            else:
                content = str(response)
            
            chat_history.append({"role": "assistant", "content": content})
            console.print(f"[bold magenta]Assistant:[/bold magenta] {content}")
            
        except KeyboardInterrupt:
            console.print("\n[bold yellow]Chat interrupted.[/bold yellow]")
            break
        except Exception as e:
            console.print(f"[bold red]Error:[/bold red] {str(e)}")


@app.command()
def call(
    config_file: Path = typer.Argument(..., help="Path to configuration file (JSON or YAML)"),
    prompt: Optional[str] = typer.Option(None, "--prompt", "-p", help="Override prompt from config"),
    model: Optional[str] = typer.Option(None, "--model", "-m", help="Override model from config"),
    validate: Optional[List[str]] = typer.Option(None, "--validate", "-v", help="Override validation"),
    show_config: bool = typer.Option(False, "--show-config", help="Display final configuration")
):
    """
    Make an LLM call using a configuration file with optional overrides.
    
    Examples:
    - Basic: call config.json
    - Override prompt: call config.json --prompt "New prompt"
    - Override model: call config.json --model gpt-4
    - Show config: call config.json --show-config
    """
    import asyncio
    from llm_call.core.caller import make_llm_request
    
    try:
        # Build configuration
        config = build_llm_config(
            prompt=prompt or "",
            model=model,
            validation=validate,
            config_file=config_file
        )
        
        if show_config:
            console.print(Panel(
                Syntax(json.dumps(config, indent=2), "json"),
                title="Final Configuration",
                border_style="blue"
            ))
        
        # Make the request
        with console.status("[bold green]Processing..."):
            response = asyncio.run(make_llm_request(config))
        
        # Display response
        if isinstance(response, dict):
            console.print(Panel(
                Syntax(json.dumps(response, indent=2), "json"),
                title="Response",
                border_style="green"
            ))
        else:
            console.print(Panel(str(response), title="Response", border_style="green"))
            
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        raise typer.Exit(1)

@app.command()
def models(
    provider: Optional[str] = typer.Option(None, "--provider", "-p", help="Filter by provider"),
    show_all: bool = typer.Option(False, "--all", "-a", help="Show all available models")
):
    """
    List available LLM models.
    
    Examples:
    - List all: models --all
    - Filter by provider: models --provider openai
    """
    try:
        # Common models grouped by provider
        common_models = {
            "OpenAI": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
            "Anthropic": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku", "claude-opus-4-20250514"],
            "Google": ["gemini-pro", "gemini-pro-vision"],
            "Local": ["ollama/llama2", "ollama/mistral"],
            "Claude CLI": ["max/claude-3-opus", "max/claude-3-sonnet", "max/claude-3-haiku", "max/claude-opus-4-20250514"]
        }
        
        console.print("[bold]Available Models:[/bold]")
        
        for provider_name, model_list in common_models.items():
            if provider and provider.lower() not in provider_name.lower():
                continue
                
            console.print(f"\n[bold cyan]{provider_name}:[/bold cyan]")
            for model in model_list:
                console.print(f"  â€¢ {model}")
                
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        raise typer.Exit(1)


@app.command()
def validators():
    """List available validation strategies."""
    try:
        from llm_call.core.strategies import VALIDATION_STRATEGIES
        
        console.print("[bold]Available Validation Strategies:[/bold]\n")
        
        for name, strategy in VALIDATION_STRATEGIES.items():
            console.print(f"[bold cyan]{name}:[/bold cyan]")
            if hasattr(strategy, "__doc__") and strategy.__doc__:
                console.print(f"  {strategy.__doc__.strip()}")
            console.print()
    except ImportError:
        # Fallback list if strategies module not available
        strategies = ["json", "code", "schema", "length", "contains", "regex"]
        console.print("[bold]Available Validation Strategies:[/bold]\n")
        for strategy in strategies:
            console.print(f"â€¢ {strategy}")


@app.command()
def config_example(
    output: Optional[Path] = typer.Option(None, "--output", "-o", help="Save to file"),
    format: str = typer.Option("json", "--format", "-f", help="Output format (json/yaml)")
):
    """Generate an example configuration file."""
    example_config = {
        "model": "gpt-4",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Your prompt here"}
        ],
        "temperature": 0.7,
        "max_tokens": 2000,
        "validation": [
            {"strategy": "json", "max_attempts": 3},
            {"strategy": "code", "language": "python"}
        ],
        "retry_config": {
            "max_attempts": 3,
            "backoff_factor": 2.0
        },
        "response_format": {
            "type": "json_object"
        }
    }
    
    if format == "yaml":
        content = yaml.dump(example_config, default_flow_style=False)
        syntax_format = "yaml"
    else:
        content = json.dumps(example_config, indent=2)
        syntax_format = "json"
    
    if output:
        output.write_text(content)
        console.print(f"[bold green]Example configuration saved to:[/bold green] {output}")
    else:
        console.print(Panel(
            Syntax(content, syntax_format),
            title=f"Example Configuration ({format.upper()})",
            border_style="blue"
        ))

# ============================================
# AUTO-GENERATION COMMANDS
# ============================================

@app.command()
def generate_claude(
    output_dir: Optional[Path] = typer.Option(None, "--output", "-o", help="Output directory"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose output")
):
    """
    Generate Claude Code slash commands for all CLI commands.
    
    Creates .json files that Claude Code can use as slash commands with full configuration support.
    
    Examples:
    - Basic: generate-claude
    - Custom output: generate-claude --output .claude/my-commands
    """
    # Determine output directory
    output_path = output_dir or Path(".claude/commands")
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Commands to skip
    skip_commands = {"generate-claude", "serve-mcp", "generate-mcp-config"}
    
    generated = 0
    
    for command in app.registered_commands:
        cmd_name = command.name or command.callback.__name__
        
        if cmd_name in skip_commands:
            continue
            
        # Get function details
        func = command.callback
        docstring = func.__doc__ or f"Run {cmd_name} command"
        
        # Clean up docstring
        doc_lines = docstring.strip().split('\n')
        short_description = doc_lines[0]
        
        # Generate Claude Code slash command config
        slash_config = {
            "name": f"llm-{cmd_name}",
            "description": short_description,
            "args": [],
            "execute": f"llm-cli {cmd_name}",
            "type": "command"
        }
        
        # Get function signature for arguments
        try:
            sig = inspect.signature(func)
            for param_name, param in sig.parameters.items():
                if param_name in ['self', 'ctx']:
                    continue
                    
                arg_config = {
                    "name": param_name,
                    "type": "string",
                    "optional": param.default != param.empty
                }
                
                if param.annotation != param.empty:
                    if param.annotation == int:
                        arg_config["type"] = "number"
                    elif param.annotation == bool:
                        arg_config["type"] = "boolean"
                
                slash_config["args"].append(arg_config)
        except Exception as e:
            if verbose:
                console.print(f"[yellow]Warning: Could not parse signature for {cmd_name}: {e}[/yellow]")
        
        # Write command file
        cmd_file = output_path / f"{slash_config['name']}.json"
        with open(cmd_file, 'w') as f:
            json.dump(slash_config, f, indent=2)
        
        if verbose:
            console.print(f"âœ… Created: {cmd_file}")
        
        generated += 1
    
    console.print(f"[bold green]âœ… Generated {generated} slash commands in {output_path}/[/bold green]")
    console.print("\n[bold]Available commands:[/bold]")
    
    # List generated commands
    for json_file in output_path.glob("llm-*.json"):
        with open(json_file) as f:
            config = json.load(f)
            cmd_name = config.get("name", json_file.stem)
            description = config.get("description", "No description")
            console.print(f"  [cyan]/{cmd_name}[/cyan] - {description}")

@app.command()
def generate_mcp_config(
    output: Path = typer.Option("mcp_config.json", "--output", "-o", help="Output file"),
    name: Optional[str] = typer.Option(None, "--name", help="MCP server name"),
    host: str = typer.Option("localhost", "--host", help="Server host"),
    port: int = typer.Option(5000, "--port", help="Server port")
):
    """
    Generate MCP (Model Context Protocol) configuration.
    
    Creates a configuration file for using this CLI as an MCP server
    with Claude or other MCP-compatible tools.
    """
    import inspect
    
    server_name = name or "llm-cli"
    
    # Build tool definitions
    tools = {}
    skip_commands = {"generate-claude", "serve-mcp", "generate-mcp-config"}
    
    for command in app.registered_commands:
        cmd_name = command.name or command.callback.__name__
        
        if cmd_name in skip_commands:
            continue
            
        func = command.callback
        docstring = func.__doc__ or f"Execute {cmd_name}"
        
        # Extract parameters
        sig = inspect.signature(func)
        parameters = {}
        required = []
        
        for param_name, param in sig.parameters.items():
            if param_name in ['self', 'ctx']:
                continue
                
            # Determine type
            param_type = "string"
            if param.annotation != param.empty:
                if param.annotation == int:
                    param_type = "integer"
                elif param.annotation == bool:
                    param_type = "boolean"
                elif param.annotation == float:
                    param_type = "number"
                    
            parameters[param_name] = {
                "type": param_type,
                "description": f"Parameter: {param_name}"
            }
            
            if param.default == param.empty:
                required.append(param_name)
        
        tools[cmd_name] = {
            "description": docstring.strip().split('\n')[0],
            "inputSchema": {
                "type": "object",
                "properties": parameters,
                "required": required
            }
        }
    
    # Build configuration
    config = {
        "name": server_name,
        "version": "1.0.0",
        "description": f"MCP server for {server_name}",
        "server": {
            "command": sys.executable,
            "args": ["-m", "llm_call.cli.main", "serve-mcp", "--host", host, "--port", str(port)]
        },
        "tools": tools,
        "capabilities": {
            "tools": True,
            "prompts": False,
            "resources": False
        }
    }
    
    output.write_text(json.dumps(config, indent=2))
    typer.echo(f"âœ… Generated MCP config: {output}")
    typer.echo(f"ðŸ“‹ Includes {len(tools)} tools")

@app.command()
def serve_mcp(
    host: str = typer.Option("localhost", "--host", help="Server host"),
    port: int = typer.Option(5000, "--port", help="Server port"),
    debug: bool = typer.Option(False, "--debug", help="Debug mode")
):
    """
    Serve this CLI as an MCP server using FastMCP.
    
    Allows Claude and other MCP-compatible tools to use CLI commands.
    """
    try:
        from fastmcp import FastMCP
    except ImportError:
        typer.echo("âŒ FastMCP not installed!")
        typer.echo("\nInstall with:")
        typer.echo("  pip install fastmcp")
        raise typer.Exit(1)
    
    # Create FastMCP server
    mcp = FastMCP("llm-cli")
    
    # Skip these commands
    skip_commands = {"generate-claude", "serve-mcp", "generate-mcp-config"}
    
    # Register commands as tools
    registered = 0
    
    for command in app.registered_commands:
        cmd_name = command.name or command.callback.__name__
        
        if cmd_name in skip_commands:
            continue
            
        # Note: In production, properly handle async conversion
        # This is a simplified example
        registered += 1
        
        if debug:
            typer.echo(f"  Registered: {cmd_name}")
    
    typer.echo(f"ðŸ”§ Registered {registered} MCP tools")
    typer.echo(f"ðŸš€ Starting MCP server on {host}:{port}")
    typer.echo("\nPress Ctrl+C to stop")
    
    # In production, start the actual server
    typer.echo("\n[FastMCP server would run here - install fastmcp first]")

# ============================================
# TEST RUNNER COMMANDS
# ============================================

@app.command()
def test(
    pattern: Optional[str] = typer.Argument(None, help="Pattern to match test files (e.g., 'poc_*.py', 'test_*.py')"),
    directory: Optional[Path] = typer.Option(None, "--dir", "-d", help="Directory to search for tests"),
    parallel: bool = typer.Option(False, "--parallel", "-p", help="Run tests in parallel"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose output"),
    timeout: int = typer.Option(30, "--timeout", "-t", help="Timeout per test in seconds"),
    show_output: bool = typer.Option(False, "--show-output", "-o", help="Show test output even on success")
):
    """
    Run POC validation tests and report results.
    
    Examples:
    - Run all POCs: test "poc_*.py"
    - Run specific POC: test "poc_11_*.py"
    - Run all tests: test "test_*.py"
    - Run in parallel: test "poc_*.py" --parallel
    - Custom directory: test --dir src/llm_call/proof_of_concept
    """
    import subprocess
    import glob
    import asyncio
    from concurrent.futures import ThreadPoolExecutor
    from pathlib import Path
    import time
    
    # Default search directory
    if directory:
        search_dir = directory
    else:
        # Try common POC locations
        poc_dirs = [
            Path("src/llm_call/proof_of_concept"),
            Path("src/llm_call/proof_of_concept/code/task_004_test_prompts"),
            Path("tests/llm_call/core"),
            Path(".")
        ]
        
        # Find first existing directory
        search_dir = None
        for d in poc_dirs:
            if d.exists():
                search_dir = d
                break
        
        if not search_dir:
            console.print("[bold red]Error:[/bold red] No test directory found")
            raise typer.Exit(1)
    
    # Default pattern if not provided
    if not pattern:
        pattern = "poc_*.py"
    
    # Find test files
    test_files = []
    
    # Handle recursive search
    if "**" in pattern:
        test_files = list(search_dir.glob(pattern))
    else:
        # Search in directory and immediate subdirectories
        test_files = list(search_dir.glob(pattern))
        for subdir in search_dir.iterdir():
            if subdir.is_dir():
                test_files.extend(subdir.glob(pattern))
    
    if not test_files:
        console.print(f"[bold yellow]No test files found matching '{pattern}' in {search_dir}[/bold yellow]")
        raise typer.Exit(0)
    
    console.print(f"[bold]Found {len(test_files)} test files in {search_dir}[/bold]")
    
    # Test runner function
    def run_test_file(test_file: Path) -> Dict[str, Any]:
        """Run a single test file and return results."""
        start_time = time.time()
        
        try:
            # Run the test
            result = subprocess.run(
                [sys.executable, str(test_file)],
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            elapsed = time.time() - start_time
            
            # Check for success patterns
            success_patterns = [
                "VALIDATION PASSED",
                "All tests passed",
                "âœ… VALIDATION PASSED",
                "tests produced expected results",
                "âœ… All exponential backoff tests passed",
                "âœ… All"
            ]
            
            # Check both stdout and stderr (for loguru output)
            output = result.stdout + result.stderr
            
            # Also check for failure patterns
            failure_patterns = [
                "VALIDATION FAILED",
                "tests failed",
                "âŒ VALIDATION FAILED",
                "AssertionError",
                "Exception:",
                "Traceback"
            ]
            
            has_success = any(pattern in output for pattern in success_patterns)
            has_failure = any(pattern in output for pattern in failure_patterns)
            
            # Success if: has success pattern, no failure pattern, and exit code 0
            success = has_success and not has_failure and result.returncode == 0
            
            return {
                "file": test_file.name,
                "success": success,
                "elapsed": elapsed,
                "returncode": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "output": output
            }
            
        except subprocess.TimeoutExpired:
            return {
                "file": test_file.name,
                "success": False,
                "elapsed": timeout,
                "error": f"Timeout after {timeout}s",
                "output": ""
            }
        except Exception as e:
            return {
                "file": test_file.name,
                "success": False,
                "elapsed": time.time() - start_time,
                "error": str(e),
                "output": ""
            }
    
    # Run tests
    results = []
    
    if parallel:
        console.print("[bold blue]Running tests in parallel...[/bold blue]")
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(run_test_file, f) for f in test_files]
            
            for future in futures:
                results.append(future.result())
    else:
        console.print("[bold blue]Running tests sequentially...[/bold blue]")
        for test_file in test_files:
            if verbose:
                console.print(f"\n[dim]Running {test_file.name}...[/dim]")
            
            result = run_test_file(test_file)
            results.append(result)
            
            # Show immediate feedback
            if result["success"]:
                console.print(f"âœ… {result['file']} - [green]PASSED[/green] ({result['elapsed']:.2f}s)")
                if show_output:
                    console.print(Panel(result["output"], title=f"Output: {result['file']}", border_style="green"))
            else:
                console.print(f"âŒ {result['file']} - [red]FAILED[/red] ({result['elapsed']:.2f}s)")
                if verbose or show_output:
                    error_msg = result.get("error", "Test failed")
                    console.print(Panel(
                        f"Error: {error_msg}\n\nOutput:\n{result.get('output', 'No output')}",
                        title=f"Failed: {result['file']}",
                        border_style="red"
                    ))
    
    # Summary report
    console.print("\n" + "="*60)
    console.print("[bold]Test Summary Report[/bold]")
    console.print("="*60)
    
    passed = sum(1 for r in results if r["success"])
    failed = len(results) - passed
    total_time = sum(r["elapsed"] for r in results)
    
    console.print(f"\n[bold]Total:[/bold] {len(results)} tests")
    console.print(f"[bold green]Passed:[/bold green] {passed}")
    console.print(f"[bold red]Failed:[/bold red] {failed}")
    console.print(f"[bold]Total time:[/bold] {total_time:.2f}s")
    
    if failed > 0:
        console.print("\n[bold red]Failed tests:[/bold red]")
        for r in results:
            if not r["success"]:
                console.print(f"  â€¢ {r['file']}: {r.get('error', 'Test failed')}")
    
    # Performance report
    if verbose and len(results) > 0:
        console.print("\n[bold]Performance Report:[/bold]")
        sorted_results = sorted(results, key=lambda x: x["elapsed"], reverse=True)
        
        console.print("\nSlowest tests:")
        for r in sorted_results[:5]:
            console.print(f"  â€¢ {r['file']}: {r['elapsed']:.2f}s")
        
        avg_time = total_time / len(results)
        console.print(f"\nAverage time per test: {avg_time:.2f}s")
    
    # Exit code based on results
    if failed > 0:
        raise typer.Exit(1)
    else:
        console.print("\n[bold green]âœ… All tests passed![/bold green]")


@app.command()
def test_poc(
    poc_number: Optional[int] = typer.Argument(None, help="Specific POC number to run (e.g., 11)"),
    show_all: bool = typer.Option(False, "--all", "-a", help="Show all POC files"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose output")
):
    """
    Run specific POC validation tests.
    
    Examples:
    - List all POCs: test-poc --all
    - Run specific POC: test-poc 11
    - Run POC 27: test-poc 27 --verbose
    """
    from pathlib import Path
    
    # Find POC directory
    poc_dir = Path("src/llm_call/proof_of_concept/code/task_004_test_prompts")
    if not poc_dir.exists():
        poc_dir = Path("src/llm_call/proof_of_concept")
    
    if not poc_dir.exists():
        console.print("[bold red]Error:[/bold red] POC directory not found")
        raise typer.Exit(1)
    
    # List all POCs if requested
    if show_all or poc_number is None:
        poc_files = sorted(poc_dir.glob("poc_*.py"))
        
        if not poc_files:
            console.print("[bold yellow]No POC files found[/bold yellow]")
            raise typer.Exit(0)
        
        console.print("[bold]Available POC files:[/bold]\n")
        
        # Group by number
        poc_groups = {}
        for f in poc_files:
            # Extract POC number
            import re
            match = re.match(r"poc_(\d+)", f.name)
            if match:
                num = int(match.group(1))
                if num not in poc_groups:
                    poc_groups[num] = []
                poc_groups[num].append(f)
        
        for num in sorted(poc_groups.keys()):
            console.print(f"[bold cyan]POC {num:02d}:[/bold cyan]")
            for f in poc_groups[num]:
                # Try to extract description from file
                try:
                    with open(f) as file:
                        lines = file.readlines()[:10]
                        for line in lines:
                            if "POC-" in line and ":" in line:
                                desc = line.split(":", 1)[1].strip()
                                console.print(f"  â€¢ {f.name} - {desc}")
                                break
                        else:
                            console.print(f"  â€¢ {f.name}")
                except:
                    console.print(f"  â€¢ {f.name}")
        
        if poc_number is None:
            raise typer.Exit(0)
    
    # Run specific POC
    pattern = f"poc_{poc_number:02d}_*.py" if poc_number < 100 else f"poc_{poc_number}_*.py"
    
    console.print(f"\n[bold]Running POC {poc_number}...[/bold]")
    
    # Call the test command directly
    test(
        pattern=pattern,
        directory=poc_dir,
        verbose=verbose,
        show_output=True,
        parallel=False
    )


# ============================================
# MAIN ENTRY POINT
# ============================================

if __name__ == "__main__":
    app()