#!/usr/bin/env python3
"""
Fixed version of litellm_client_poc.py that properly handles meta-task/* models.

These are not real LLM models but validation tasks that should trigger validation errors.
"""

import os
import sys
from pathlib import Path

# Import the original module
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
from src.llm_call.proof_of_concept.litellm_client_poc import *

# Override the routing function to handle meta-task/* models
original_determine_llm_route = determine_llm_route_and_params

def determine_llm_route_and_params(llm_config: Dict[str, Any], POC_CONFIG: Dict[str, Any]) -> Dict[str, Any]:
    """Enhanced routing that handles meta-task/* models."""
    model = llm_config.get("model", "")
    
    # Handle meta-task/* models specially
    if model.startswith("meta-task/"):
        logger.info(f"ðŸŽ¯ Meta-task model detected: {model}")
        # These should fail immediately as they're not real models
        # But they have validation that expects them to fail
        # So we need to let them through to the validation layer
        # Return a minimal config that will trigger validation
        return {
            "route_type": "meta-task",
            "processed_messages": llm_config.get("messages", []),
            "llm_call_kwargs": {"model": model},
            "model_name_original": model
        }
    
    # For all other models, use original routing
    return original_determine_llm_route(llm_config, POC_CONFIG)

# Override llm_call to handle meta-task routing
async def llm_call(llm_config: Dict[str, Any]) -> Any:
    """Enhanced llm_call that handles meta-task models."""
    POC_CONFIG = load_poc_configuration()
    route_info = determine_llm_route_and_params(llm_config, POC_CONFIG)
    
    # Handle meta-task specially
    if route_info["route_type"] == "meta-task":
        # These models are meant to fail and trigger validation
        # Return an error response that validation can check
        return {
            "error": f"Meta-task model '{route_info['model_name_original']}' is not a real LLM model",
            "choices": [{
                "message": {
                    "content": "This is a meta-task validation test"
                }
            }]
        }
    
    # Import original llm_call and use it
    from src.llm_call.proof_of_concept.litellm_client_poc import llm_call as original_llm_call
    return await original_llm_call(llm_config)

if __name__ == "__main__":
    # Test the fix
    import asyncio
    
    test_config = {
        "model": "meta-task/test",
        "messages": [{"role": "user", "content": "test"}]
    }
    
    result = asyncio.run(llm_call(test_config))
    print("Meta-task test result:", result)