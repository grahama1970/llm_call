# Test Report - 2025-06-05 06:15:40

## Summary
- **Total Tests**: 143
- **Passed**: 80 (55.9%)
- **Failed**: 11 (7.7%)
- **Skipped**: 52 (36.4%)
- **Duration**: 70.97s

## Test Results

| Test Name | Description | Result | Status | Duration | Timestamp | Error Message |
|-----------|-------------|--------|--------|----------|-----------|---------------|
| test_ask_basic | Test basic ask command. | Success | Pass | 3.641s | 2025-06-05 06:15:45 |  |
| test_ask_with_model | Test ask command with model selection. | Success | Pass | 3.274s | 2025-06-05 06:15:48 |  |
| test_ask_with_validation | Test ask command with validation. | Success | Pass | 3.442s | 2025-06-05 06:15:52 |  |
| test_ask_json_mode | Test ask command with JSON mode. | Success | Pass | 3.561s | 2025-06-05 06:15:55 |  |
| test_ask_with_system_prompt | Test ask command with system prompt. | Success | Pass | 3.282s | 2025-06-05 06:15:59 |  |
| test_ask_show_config | Test ask command with config display. | Success | Pass | 3.574s | 2025-06-05 06:16:02 |  |
| test_chat_basic | Test basic chat command. | Success | Pass | 0.008s | 2025-06-05 06:16:02 |  |
| test_chat_with_system | Test chat with system prompt. | Success | Pass | 0.008s | 2025-06-05 06:16:02 |  |
| test_call_json_config | Test call command with JSON config. | Success | Pass | 3.566s | 2025-06-05 06:16:06 |  |
| test_call_yaml_config | Test call command with YAML config. | Success | Pass | 3.439s | 2025-06-05 06:16:09 |  |
| test_call_with_overrides | Test call command with config overrides. | Success | Pass | 3.168s | 2025-06-05 06:16:12 |  |
| test_models_list_all | Test listing all models. | Success | Pass | 0.007s | 2025-06-05 06:16:12 |  |
| test_models_filter_provider | Test filtering models by provider. | Success | Pass | 0.004s | 2025-06-05 06:16:12 |  |
| test_validators_list | Test listing validation strategies. | Success | Pass | 0.012s | 2025-06-05 06:16:12 |  |
| test_config_example_json | Test generating example JSON config. | Success | Pass | 0.004s | 2025-06-05 06:16:12 |  |
| test_config_example_yaml | Test generating example YAML config. | Success | Pass | 0.006s | 2025-06-05 06:16:12 |  |
| test_generate_claude | Test Claude slash command generation. | Success | Pass | 0.009s | 2025-06-05 06:16:12 |  |
| test_generate_mcp_config | Test MCP config generation. | Success | Pass | 0.005s | 2025-06-05 06:16:12 |  |
| test_serve_mcp_help | Test serve-mcp command help. | Success | Pass | 0.010s | 2025-06-05 06:16:12 |  |
| test_test_command_no_files | Test 'test' command when no test files exist. | Success | Pass | 0.004s | 2025-06-05 06:16:12 |  |
| test_test_command_with_files | Test 'test' command with test files. | Success | Pass | 0.025s | 2025-06-05 06:16:13 |  |
| test_test_poc_help | Test test-poc command help. | Success | Pass | 0.009s | 2025-06-05 06:16:13 |  |
| test_readme_examples_exist | Test that commands mentioned in README exist in CLI. | Success | Pass | 0.011s | 2025-06-05 06:16:13 |  |
| test_readme_ask_examples | Test that README 'ask' examples work. | Success | Pass | 3.423s | 2025-06-05 06:16:16 |  |
| test_config_with_validation | Test using config file with validation. | Success | Pass | 3.460s | 2025-06-05 06:16:19 |  |
| test_generate_then_use_commands | Test generating commands and verifying they're usable. | Success | Pass | 0.026s | 2025-06-05 06:16:19 |  |
| test_call_missing_config | Test call command with missing config file. | Success | Pass | 0.005s | 2025-06-05 06:16:19 |  |
| test_call_invalid_config_format | Test call command with unsupported config format. | Success | Pass | 0.005s | 2025-06-05 06:16:19 |  |
| test_ask_invalid_model | Test handling invalid model. | Success | Pass | 2.892s | 2025-06-05 06:16:22 |  |
| test_cli_uses_router_real | Test that CLI properly uses the router with real LLM. | Success | Pass | 3.408s | 2025-06-05 06:16:26 |  |
| test_model_routing_patterns_real | Test different model routing patterns with real routers. | Success | Pass | 0.000s | 2025-06-05 06:16:26 |  |
| test_validation_strategies_applied_real | Test that validation strategies are properly applied with real LLM. | Success | Pass | 3.393s | 2025-06-05 06:16:29 |  |
| test_validation_with_real_response | Test validation with actual LLM response. | Skipped | Skip | 0.000s | 2025-06-05 06:16:29 |  |
| test_retry_config_in_file | Test retry configuration via config file with real execution. | Success | Pass | 1.250s | 2025-06-05 06:16:30 |  |
| test_openai_integration_real | Test OpenAI provider integration with real API. | Test failed | Fail | 3.312s | 2025-06-05 06:16:34 | tests/llm_call/cli/test_llm_integration.py:159: in test_openai_integration_real     assert "hello" i... |
| test_local_model_integration | Test local model integration. | Success | Pass | 3.310s | 2025-06-05 06:16:37 |  |
| test_streaming_disabled_by_default | Test that streaming is disabled by default. | Success | Pass | 3.511s | 2025-06-05 06:16:41 |  |
| test_config_priority_real | Test configuration priority (CLI > file) with real LLM. | Success | Pass | 3.379s | 2025-06-05 06:16:44 |  |
| test_system_prompt_handling_real | Test system prompt is properly added to messages with real LLM. | Success | Pass | 3.344s | 2025-06-05 06:16:47 |  |
| test_llm_error_propagation_real | Test that LLM errors are properly handled with invalid model. | Success | Pass | 2.818s | 2025-06-05 06:16:50 |  |
| test_validation_error_handling | Test validation error handling with real validator. | Success | Pass | 0.206s | 2025-06-05 06:16:50 |  |
| test_generate_mcp_config_structure | Test that generated MCP config has correct structure. | Success | Pass | 0.004s | 2025-06-05 06:16:50 |  |
| test_mcp_tool_definitions | Test that MCP tools are properly defined. | Success | Pass | 0.005s | 2025-06-05 06:16:50 |  |
| test_mcp_parameter_types | Test that parameter types are correctly mapped. | Success | Pass | 0.004s | 2025-06-05 06:16:50 |  |
| test_serve_mcp_initialization | Test MCP server initialization. | Test failed | Fail | 0.004s | 2025-06-05 06:16:50 | tests/llm_call/cli/test_mcp_features.py:190: in test_serve_mcp_initialization     assert result.exit... |
| test_serve_mcp_missing_dependency | Test error when FastMCP is not installed. | Success | Pass | 0.004s | 2025-06-05 06:16:50 |  |
| test_serve_mcp_debug_mode | Test MCP server in debug mode. | Test failed | Fail | 0.004s | 2025-06-05 06:16:50 | tests/llm_call/cli/test_mcp_features.py:209: in test_serve_mcp_debug_mode     assert result.exit_cod... |
| test_add_slash_mcp_commands | Test adding slash/MCP commands to a CLI app. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_generate_claude_via_mixin | Test Claude generation via mixin. | Success | Pass | 0.002s | 2025-06-05 06:16:50 |  |
| test_generate_mcp_via_mixin | Test MCP config generation via mixin. | Success | Pass | 0.002s | 2025-06-05 06:16:50 |  |
| test_slash_mcp_decorator | Test the @slash_mcp_cli decorator. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_claude_slash_command_format | Test that Claude slash commands have correct format. | Success | Pass | 0.012s | 2025-06-05 06:16:50 |  |
| test_claude_command_skip_list | Test that certain commands are skipped for Claude. | Success | Pass | 0.009s | 2025-06-05 06:16:50 |  |
| test_mcp_tool_execution_mapping | Test that MCP tools map correctly to CLI execution. | Success | Pass | 0.004s | 2025-06-05 06:16:50 |  |
| test_full_mcp_generation_workflow | Test generating both Claude and MCP configs. | Success | Pass | 0.013s | 2025-06-05 06:16:50 |  |
| test_mcp_parameter_consistency | Test that parameters are consistent across formats. | Success | Pass | 0.004s | 2025-06-05 06:16:50 |  |
| test_config_building | Test configuration building from various sources. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_config_file_loading | Test loading configurations from files. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_slash_command_generation | Test slash command configuration generation. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_validation_integration | Test validation strategy integration. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_complete_capability_verification | Run complete verification and generate summary report. | Success | Pass | 0.003s | 2025-06-05 06:16:50 |  |
| test_llm_call_delegator_tool_exists | Verify the LLM call delegator tool exists and is functional. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_mcp_configuration_supports_llm_tools | Verify MCP configuration supports LLM collaboration tools. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_router_supports_multiple_models | Verify the router can handle multiple model specifications. | Test failed | Fail | 0.000s | 2025-06-05 06:16:50 | tests/llm_call/core/test_claude_capabilities_verification.py:68: in test_router_supports_multiple_mo... |
| test_claude_to_gemini_collaboration_scenario | Test scenario where Claude would collaborate with Gemini. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_all_validators_inventory | Create inventory of all available validators. | Success | Pass | 0.001s | 2025-06-05 06:16:50 |  |
| test_json_validation_comprehensive | Test JSON validation thoroughly. | Success | Pass | 0.002s | 2025-06-05 06:16:50 |  |
| test_async_validators | Test async validation strategies. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_rl_integration_imports | Test that RL integration module exists and has correct structure. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_rl_provider_selector_structure | Verify the provider selector has the expected structure. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_final_capability_summary | Generate final summary of verified capabilities. | Success | Pass | 0.001s | 2025-06-05 06:16:50 |  |
| test_claude_calls_another_claude | Test Claude instance calling another Claude instance. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_claude_calls_gemini_1m_context | Test Claude delegating to Gemini for large context tasks. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_mcp_tool_chaining | Test Claude using multiple MCP tools in collaboration. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_recursion_protection | Test that recursive LLM calls have proper depth limits. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_model_capability_routing | Test routing to appropriate models based on capabilities. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_mcp_server_configuration | Test MCP server configuration generation. | Test failed | Fail | 0.000s | 2025-06-05 06:16:50 | tests/llm_call/core/test_claude_collaboration.py:222: in test_mcp_server_configuration     from llm_... |
| test_mcp_write_and_cleanup | Test MCP config file writing and cleanup. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_research_and_implement_workflow | Test a complete research -> implement workflow using multiple models. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_error_handling_in_collaboration | Test error handling when collaboration fails. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_llm_call_delegator_exists | Verify the LLM Call Delegator tool exists. | assert hasattr(delegator, 'execute') | Fail | 0.000s | 2025-06-05 06:16:50 | tests/llm_call/core/test_claude_collaboration_fixed.py:44: in test_llm_call_delegator_exists     ass... |
| test_mcp_configuration_for_collaboration | Test MCP configuration supports model collaboration. | Test failed | Fail | 0.000s | 2025-06-05 06:16:50 | tests/llm_call/core/test_claude_collaboration_fixed.py:60: in test_mcp_configuration_for_collaborati... |
| test_claude_to_claude_capability | Test that Claude can theoretically call another Claude instance. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_model_routing_capabilities | Test that the system can route to different models. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_large_context_delegation_scenario | Test scenario where Claude would delegate to Gemini for large context. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_mcp_tools_availability | Test that collaboration-enabling MCP tools are available. | Success | Pass | 0.000s | 2025-06-05 06:16:50 |  |
| test_delegator_recursion_protection | Test that the delegator has recursion protection. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_collaboration_workflow_design | Test a theoretical multi-model collaboration workflow. | Success | Pass | 0.001s | 2025-06-05 06:16:50 |  |
| test_mcp_config_write_and_cleanup | Test writing and cleaning up MCP config files. | assert config_path.name == "mcp_config.json" | Fail | 0.000s | 2025-06-05 06:16:50 | tests/llm_call/core/test_claude_collaboration_fixed.py:251: in test_mcp_config_write_and_cleanup    ... |
| test_polling_mode_returns_task_id | Test that polling mode returns immediately with task_id. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_polling_status_endpoint | Test checking status via polling endpoint. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_wait_for_completion | Test waiting for task completion without polling mode. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_progress_tracking | Test that progress updates are tracked during execution. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_concurrent_tasks | Test multiple concurrent polling tasks. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_task_cancellation | Test cancelling a running task. | Skipped | Skip | 0.000s | 2025-06-05 06:16:50 |  |
| test_sqlite_persistence | Test that tasks are persisted in SQLite database. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_all_validators_registered | Ensure all validators are properly registered. | Test failed | Fail | 0.000s | 2025-06-05 06:16:51 | tests/llm_call/core/test_comprehensive_validation.py:52: in test_all_validators_registered     strat... |
| test_validator_edge_cases[test_case0] | Validator edge cases[case0] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case1] | Validator edge cases[case1] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case2] | Validator edge cases[case2] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case3] | Validator edge cases[case3] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case4] | Validator edge cases[case4] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case5] | Validator edge cases[case5] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case6] | Validator edge cases[case6] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case7] | Validator edge cases[case7] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case8] | Validator edge cases[case8] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case9] | Validator edge cases[case9] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case10] | Validator edge cases[case10] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case11] | Validator edge cases[case11] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case12] | Validator edge cases[case12] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case13] | Validator edge cases[case13] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case14] | Validator edge cases[case14] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case15] | Validator edge cases[case15] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case16] | Validator edge cases[case16] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case17] | Validator edge cases[case17] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case18] | Validator edge cases[case18] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_edge_cases[test_case19] | Validator edge cases[case19] | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_json_schema_validation | Test JSON schema validation with complex schemas. | Success | Pass | 0.004s | 2025-06-05 06:16:51 |  |
| test_field_presence_validation | Test field presence validation including nested fields. | assert result["valid"] == test["should_pass"], \ | Fail | 0.000s | 2025-06-05 06:16:51 | tests/llm_call/core/test_comprehensive_validation.py:183: in test_field_presence_validation     asse... |
| test_ai_validators_with_mock | Test AI validators with mocked LLM responses. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_openapi_spec_validation | Test OpenAPI specification validation. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_code_validator_with_language | Test code validation with language specification. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_response_not_empty_validator | Test response not empty validator with various inputs. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_json_string_validator | Test JSON string validator (ensures response is valid JSON string). | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validator_chaining | Test chaining multiple validators together. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_malicious_input_handling | Test validators handle malicious inputs safely. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validation_in_retry_manager | Test validation works correctly in retry scenarios. | Test failed | Fail | 0.002s | 2025-06-05 06:16:51 | tests/llm_call/core/test_comprehensive_validation.py:364: in test_validation_in_retry_manager     co... |
| test_validation_error_messages | Test validators provide helpful error messages. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_basic_llm_request | Test basic LLM request with real provider. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_json_response_format | Test JSON response format with real LLM. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_router_integration | Test router correctly selects providers. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_validation_integration | Test validation with real LLM responses. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_error_handling | Test error handling with invalid model. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_temperature_effects | Test temperature parameter effects on responses. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_system_message_handling | Test system message handling. | Skipped | Skip | 0.000s | 2025-06-05 06:16:51 |  |
| test_format_detection | Test enhanced format detection. | Success | Pass | 0.022s | 2025-06-05 06:16:51 |  |
| test_api_formatting | Test API-specific formatting. | Success | Pass | 0.000s | 2025-06-05 06:16:51 |  |
| test_metadata_encoding | Test image encoding with metadata. | Success | Pass | 0.003s | 2025-06-05 06:16:51 |  |
| test_integration | Test integration with existing process_image_input. | Success | Pass | 0.002s | 2025-06-05 06:16:51 |  |
| test_model_parsing_in_executor | Test that model names are correctly parsed and added to CLI command. | Success | Pass | 0.016s | 2025-06-05 06:16:51 |  |
| test_router_to_executor_flow | Test the complete flow from router to executor. | Success | Pass | 0.000s | 2025-06-05 06:16:51 |  |
| test_model_extraction_logic[max/opus-opus] | test_model_extraction_logic[max/opus-opus].__doc__.strip().split | Success | Pass | 0.000s | 2025-06-05 06:16:51 |  |
| test_model_extraction_logic[max/sonnet-sonnet] | test_model_extraction_logic[max/sonnet-sonnet].__doc__.strip().split | Success | Pass | 0.000s | 2025-06-05 06:16:51 |  |

## Test Distribution by Module

| Module | Total | Passed | Failed | Skipped |
|--------|-------|--------|--------|---------|
| tests/llm_call/cli/test_cli_comprehensive.py | 29 | 29 | 0 | 0 |
| tests/llm_call/cli/test_llm_integration.py | 12 | 10 | 1 | 1 |
| tests/llm_call/cli/test_mcp_features.py | 15 | 13 | 2 | 0 |
| tests/llm_call/cli/test_unified_integration.py | 4 | 4 | 0 | 0 |
| tests/llm_call/core/test_capabilities_final_verification.py | 1 | 1 | 0 | 0 |
| tests/llm_call/core/test_claude_capabilities_verification.py | 10 | 8 | 1 | 1 |
| tests/llm_call/core/test_claude_collaboration.py | 9 | 2 | 1 | 6 |
| tests/llm_call/core/test_claude_collaboration_fixed.py | 9 | 4 | 3 | 2 |
| tests/llm_call/core/test_claude_proxy_polling.py | 7 | 0 | 0 | 7 |
| tests/llm_call/core/test_comprehensive_validation.py | 32 | 1 | 3 | 28 |
| tests/llm_call/core/test_core_integration.py | 7 | 0 | 0 | 7 |
| tests/llm_call/core/test_image_encoding_enhancements.py | 4 | 4 | 0 | 0 |
| tests/llm_call/core/test_max_model_routing_functional.py | 4 | 4 | 0 | 0 |