"""
Comprehensive test suite for RL (Reinforcement Learning) integration.

This tests the r1_commons reward system integration to ensure:
1. Provider selection based on performance
2. Reward tracking and learning
3. Contextual bandit functionality
4. Safe deployment with fallbacks
"""

import pytest
import numpy as np
from unittest.mock import patch, MagicMock
from pathlib import Path
import json
import time

# Try to import RL components
try:
    from llm_call.rl_integration.provider_selector import (
        RLProviderSelector, ProviderMetrics
    )
    from llm_call.rl_integration.integration_example import (
        RLEnhancedLLMClient, SafeRLDeployment
    )
    RL_AVAILABLE = True
except ImportError:
    RL_AVAILABLE = False
    pytest.skip("RL integration not available", allow_module_level=True)


class TestProviderMetrics:
    """Test provider performance metrics tracking."""
    
    def test_metrics_initialization(self):
        """Test metrics are properly initialized."""
        metrics = ProviderMetrics("gpt-4")
        
        assert metrics.provider_name == "gpt-4"
        assert metrics.total_requests == 0
        assert metrics.successful_requests == 0
        assert metrics.error_count == 0
        assert metrics.success_rate == 0.0
        assert metrics.avg_latency == 0.0
        assert metrics.avg_quality == 0.5  # Default quality
    
    def test_metrics_calculation(self):
        """Test metric calculations are correct."""
        metrics = ProviderMetrics("claude-3")
        
        # Simulate some requests
        metrics.total_requests = 100
        metrics.successful_requests = 90
        metrics.error_count = 10
        metrics.total_latency = 450.0  # 5 seconds average
        metrics.quality_scores = [0.8, 0.9, 0.7, 0.85, 0.95]
        metrics.total_cost = 4.5
        
        assert metrics.success_rate == 0.9
        assert metrics.avg_latency == 5.0
        assert np.isclose(metrics.avg_quality, 0.84)
        assert metrics.avg_cost_per_request == 0.05
    
    def test_edge_cases(self):
        """Test metric calculations handle edge cases."""
        metrics = ProviderMetrics("test-provider")
        
        # No requests yet
        assert metrics.success_rate == 0.0
        assert metrics.avg_latency == 0.0
        assert metrics.avg_cost_per_request == 0.0
        
        # Only failed requests
        metrics.total_requests = 10
        metrics.successful_requests = 0
        metrics.error_count = 10
        
        assert metrics.success_rate == 0.0
        assert metrics.avg_latency == 0.0  # No successful requests to measure


class TestRLProviderSelector:
    """Test RL-based provider selection."""
    
    @pytest.fixture
    def mock_rl_commons(self):
        """Mock the graham_rl_commons components."""
        with patch('llm_call.rl_integration.provider_selector.ContextualBandit') as mock_bandit, \
             patch('llm_call.rl_integration.provider_selector.RLState') as mock_state, \
             patch('llm_call.rl_integration.provider_selector.RLAction') as mock_action, \
             patch('llm_call.rl_integration.provider_selector.RLReward') as mock_reward:
            
            # Mock bandit behavior
            mock_bandit_instance = MagicMock()
            mock_bandit_instance.select_action.return_value = 0  # Select first provider
            mock_bandit.return_value = mock_bandit_instance
            
            yield {
                'bandit': mock_bandit_instance,
                'state_class': mock_state,
                'action_class': mock_action,
                'reward_class': mock_reward
            }
    
    def test_provider_selection(self, mock_rl_commons):
        """Test provider selection based on context."""
        selector = RLProviderSelector(
            providers=["gpt-4", "claude-3", "gemini-pro"],
            exploration_rate=0.1
        )
        
        # Create context
        context = {
            "task_type": "code_generation",
            "complexity": "high",
            "max_tokens": 2000,
            "requires_reasoning": True
        }
        
        # Select provider
        selected = selector.select_provider(context)
        
        assert selected in ["gpt-4", "claude-3", "gemini-pro"]
        assert mock_rl_commons['bandit'].select_action.called
    
    def test_reward_update(self, mock_rl_commons):
        """Test updating rewards based on performance."""
        selector = RLProviderSelector(
            providers=["gpt-4", "claude-3"],
            exploration_rate=0.0  # No exploration for testing
        )
        
        # Simulate a successful request
        provider = "gpt-4"
        context = {"task_type": "analysis"}
        performance = {
            "success": True,
            "latency": 2.5,
            "quality_score": 0.9,
            "cost": 0.05
        }
        
        # Update with reward
        selector.update_reward(provider, context, performance)
        
        # Verify bandit was updated
        assert mock_rl_commons['bandit'].update.called
        
        # Check metrics were updated
        metrics = selector.provider_metrics[provider]
        assert metrics.total_requests == 1
        assert metrics.successful_requests == 1
        assert metrics.total_latency == 2.5
    
    def test_exploration_vs_exploitation(self, mock_rl_commons):
        """Test exploration vs exploitation balance."""
        selector = RLProviderSelector(
            providers=["gpt-4", "claude-3", "gemini-pro"],
            exploration_rate=1.0  # Always explore
        )
        
        # With 100% exploration, should randomly select
        selections = []
        for _ in range(100):
            selected = selector.select_provider({"task": "test"})
            selections.append(selected)
        
        # Should have selected all providers at least once
        unique_selections = set(selections)
        assert len(unique_selections) > 1
    
    def test_performance_based_selection(self):
        """Test that better performing providers are selected more often."""
        selector = RLProviderSelector(
            providers=["good-provider", "bad-provider"],
            exploration_rate=0.0
        )
        
        # Simulate performance history
        # Good provider: high success, low latency
        for _ in range(10):
            selector.update_reward(
                "good-provider",
                {"task": "test"},
                {"success": True, "latency": 1.0, "quality_score": 0.9, "cost": 0.05}
            )
        
        # Bad provider: low success, high latency
        for _ in range(10):
            selector.update_reward(
                "bad-provider",
                {"task": "test"},
                {"success": False, "latency": 10.0, "quality_score": 0.3, "cost": 0.10}
            )
        
        # Check metrics reflect performance
        good_metrics = selector.provider_metrics["good-provider"]
        bad_metrics = selector.provider_metrics["bad-provider"]
        
        assert good_metrics.success_rate > bad_metrics.success_rate
        assert good_metrics.avg_latency < bad_metrics.avg_latency
        assert good_metrics.avg_quality > bad_metrics.avg_quality


class TestRLEnhancedLLMClient:
    """Test the RL-enhanced LLM client."""
    
    @pytest.fixture
    def client(self):
        """Create a test client."""
        return RLEnhancedLLMClient(
            providers=["gpt-3.5-turbo", "claude-instant"],
            exploration_rate=0.1
        )
    
    @pytest.mark.asyncio
    async def test_adaptive_request(self, client):
        """Test adaptive request routing."""
        with patch('llm_call.core.caller.make_llm_request') as mock_request:
            # Mock successful response
            mock_response = MagicMock()
            mock_response.model_dump.return_value = {
                "choices": [{"message": {"content": "Test response"}}],
                "usage": {"total_tokens": 100}
            }
            mock_request.return_value = mock_response
            
            # Make request
            response = await client.adaptive_request(
                prompt="Test prompt",
                task_type="simple_question"
            )
            
            assert response is not None
            assert mock_request.called
            
            # Verify provider was selected
            call_args = mock_request.call_args[0][0]
            assert call_args["model"] in ["gpt-3.5-turbo", "claude-instant"]
    
    @pytest.mark.asyncio
    async def test_fallback_on_failure(self, client):
        """Test fallback to another provider on failure."""
        with patch('llm_call.core.caller.make_llm_request') as mock_request:
            # First call fails, second succeeds
            mock_request.side_effect = [
                Exception("Provider failed"),
                MagicMock(model_dump=lambda: {"choices": [{"message": {"content": "Success"}}]})
            ]
            
            # Should retry with different provider
            response = await client.adaptive_request(
                prompt="Test prompt",
                task_type="critical_task"
            )
            
            assert response is not None
            assert mock_request.call_count >= 2
    
    def test_performance_tracking(self, client):
        """Test that performance is tracked correctly."""
        # Simulate multiple requests
        test_performance = [
            {"provider": "gpt-3.5-turbo", "success": True, "latency": 1.5},
            {"provider": "gpt-3.5-turbo", "success": True, "latency": 2.0},
            {"provider": "claude-instant", "success": False, "latency": 5.0},
            {"provider": "claude-instant", "success": True, "latency": 3.0},
        ]
        
        for perf in test_performance:
            client.selector.update_reward(
                perf["provider"],
                {"task": "test"},
                {
                    "success": perf["success"],
                    "latency": perf["latency"],
                    "quality_score": 0.8 if perf["success"] else 0.2,
                    "cost": 0.01
                }
            )
        
        # Check tracked metrics
        gpt_metrics = client.selector.provider_metrics["gpt-3.5-turbo"]
        claude_metrics = client.selector.provider_metrics["claude-instant"]
        
        assert gpt_metrics.success_rate == 1.0
        assert claude_metrics.success_rate == 0.5
        assert gpt_metrics.avg_latency < claude_metrics.avg_latency


class TestSafeRLDeployment:
    """Test safe RL deployment with gradual rollout."""
    
    def test_traffic_splitting(self):
        """Test traffic is split correctly between RL and baseline."""
        deployment = SafeRLDeployment(
            rl_percentage=30,  # 30% to RL
            baseline_provider="gpt-3.5-turbo"
        )
        
        # Simulate 1000 requests
        rl_count = 0
        baseline_count = 0
        
        for _ in range(1000):
            if deployment.should_use_rl():
                rl_count += 1
            else:
                baseline_count += 1
        
        # Check split is approximately correct (with some tolerance)
        rl_ratio = rl_count / 1000
        assert 0.25 < rl_ratio < 0.35  # 30% Â± 5%
    
    @pytest.mark.asyncio
    async def test_gradual_rollout(self):
        """Test gradual increase in RL traffic."""
        deployment = SafeRLDeployment(
            rl_percentage=10,  # Start at 10%
            baseline_provider="gpt-3.5-turbo"
        )
        
        # Simulate good performance
        for _ in range(100):
            deployment.record_performance(
                is_rl=True,
                success=True,
                latency=1.0
            )
        
        # RL should be performing well
        initial_percentage = deployment.rl_percentage
        deployment.adjust_rl_percentage()
        
        # Percentage should increase (in real implementation)
        # For now, just verify it doesn't crash
        assert deployment.rl_percentage >= initial_percentage
    
    def test_rollback_on_poor_performance(self):
        """Test rollback when RL performs poorly."""
        deployment = SafeRLDeployment(
            rl_percentage=50,
            baseline_provider="gpt-3.5-turbo"
        )
        
        # Simulate poor RL performance
        for _ in range(50):
            deployment.record_performance(
                is_rl=True,
                success=False,
                latency=10.0
            )
        
        # Simulate good baseline performance
        for _ in range(50):
            deployment.record_performance(
                is_rl=False,
                success=True,
                latency=2.0
            )
        
        # Check that we can detect poor performance
        # In real implementation, this would trigger rollback
        assert deployment.rl_percentage <= 50


class TestRLIntegrationScenarios:
    """Test real-world RL integration scenarios."""
    
    @pytest.mark.asyncio
    async def test_cost_optimization_scenario(self):
        """Test optimizing for cost while maintaining quality."""
        client = RLEnhancedLLMClient(
            providers=["gpt-4", "gpt-3.5-turbo", "claude-instant"],
            exploration_rate=0.2
        )
        
        # Define cost/quality profiles
        provider_profiles = {
            "gpt-4": {"cost": 0.10, "quality": 0.95},
            "gpt-3.5-turbo": {"cost": 0.02, "quality": 0.80},
            "claude-instant": {"cost": 0.01, "quality": 0.75}
        }
        
        # Simulate requests with cost optimization
        for _ in range(100):
            provider = "gpt-3.5-turbo"  # Simulate selection
            profile = provider_profiles[provider]
            
            client.selector.update_reward(
                provider,
                {"optimize_for": "cost"},
                {
                    "success": True,
                    "latency": 2.0,
                    "quality_score": profile["quality"],
                    "cost": profile["cost"]
                }
            )
        
        # Verify cost tracking
        for provider, metrics in client.selector.provider_metrics.items():
            if metrics.total_requests > 0:
                assert metrics.avg_cost_per_request > 0
    
    def test_multi_objective_optimization(self):
        """Test balancing multiple objectives (speed, cost, quality)."""
        selector = RLProviderSelector(
            providers=["fast-cheap", "slow-quality", "balanced"],
            exploration_rate=0.1
        )
        
        # Define provider characteristics
        scenarios = [
            {
                "provider": "fast-cheap",
                "latency": 0.5,
                "cost": 0.01,
                "quality": 0.6
            },
            {
                "provider": "slow-quality",
                "latency": 5.0,
                "cost": 0.10,
                "quality": 0.95
            },
            {
                "provider": "balanced",
                "latency": 2.0,
                "cost": 0.05,
                "quality": 0.85
            }
        ]
        
        # Simulate various task requirements
        task_types = ["urgent", "quality-critical", "cost-sensitive"]
        
        for task_type in task_types:
            # In real implementation, RL would learn optimal provider for each task type
            assert task_type in ["urgent", "quality-critical", "cost-sensitive"]
    
    def test_failure_recovery(self):
        """Test system recovers from provider failures."""
        client = RLEnhancedLLMClient(
            providers=["reliable", "flaky"],
            exploration_rate=0.1
        )
        
        # Simulate provider behavior
        # Reliable: 95% success
        for i in range(100):
            client.selector.update_reward(
                "reliable",
                {"task": "test"},
                {
                    "success": i % 20 != 0,  # Fail 5% of time
                    "latency": 2.0,
                    "quality_score": 0.9,
                    "cost": 0.05
                }
            )
        
        # Flaky: 50% success
        for i in range(100):
            client.selector.update_reward(
                "flaky",
                {"task": "test"},
                {
                    "success": i % 2 == 0,  # Fail 50% of time
                    "latency": 1.0,
                    "quality_score": 0.8,
                    "cost": 0.02
                }
            )
        
        # Verify metrics reflect reliability
        reliable_metrics = client.selector.provider_metrics["reliable"]
        flaky_metrics = client.selector.provider_metrics["flaky"]
        
        assert reliable_metrics.success_rate > flaky_metrics.success_rate
        assert reliable_metrics.error_count < flaky_metrics.error_count


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])