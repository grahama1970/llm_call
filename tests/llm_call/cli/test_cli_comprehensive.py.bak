#!/usr/bin/env python3
"""
Comprehensive CLI Testing Suite

This test suite verifies that all CLI commands and features work correctly,
including MCP integration and alignment with README documentation.

Test categories:
1. Core LLM Commands (ask, chat, call, models, validators)
2. Configuration Management (config files, overrides)
3. Auto-generation Commands (generate-claude, generate-mcp-config)
4. MCP Server Features (serve-mcp)
5. Test Runner Commands (test, test-poc)
6. README Alignment Verification
"""

import pytest
import json
import yaml
from pathlib import Path
from typer.testing import CliRunner
from unittest.mock import patch, MagicMock, AsyncMock
import tempfile
import sys

# Import the CLI app
from llm_call.cli.main import app

runner = CliRunner()


# ============================================
# FIXTURES
# ============================================

@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def sample_config_json(temp_dir):
    """Create a sample JSON config file."""
    config = {
        "model": "gpt-4",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Test prompt"}
        ],
        "temperature": 0.7,
        "max_tokens": 100,
        "validation": [
            {"type": "json"}
        ]
    }
    config_file = temp_dir / "test_config.json"
    config_file.write_text(json.dumps(config, indent=2))
    return config_file


@pytest.fixture
def sample_config_yaml(temp_dir):
    """Create a sample YAML config file."""
    config = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "user", "content": "Test YAML prompt"}
        ],
        "temperature": 0.5
    }
    config_file = temp_dir / "test_config.yaml"
    config_file.write_text(yaml.dump(config))
    return config_file


# ============================================
# CORE LLM COMMAND TESTS
# ============================================

class TestAskCommand:
    """Test the 'ask' command functionality."""
    
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_basic(self, mock_llm):
        """Test basic ask command."""
        mock_llm.return_value = {"content": "Test response"}
        
        result = runner.invoke(app, ["ask", "What is Python?"])
        
        assert result.exit_code == 0
        assert "Test response" in result.output
        mock_llm.assert_called_once()
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_with_model(self, mock_llm):
        """Test ask command with model selection."""
        mock_llm.return_value = {"content": "GPT-4 response"}
        
        result = runner.invoke(app, ["ask", "Explain AI", "--model", "gpt-4"])
        
        assert result.exit_code == 0
        assert "Using model: gpt-4" in result.output
        
        # Check the call arguments
        call_args = mock_llm.call_args[0][0]
        assert call_args["model"] == "gpt-4"
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_with_validation(self, mock_llm):
        """Test ask command with validation."""
        mock_llm.return_value = {"content": '{"key": "value"}'}
        
        result = runner.invoke(app, ["ask", "Generate JSON", "--validate", "json"])
        
        assert result.exit_code == 0
        assert "Applying validation: json" in result.output
        
        # Check validation was added
        call_args = mock_llm.call_args[0][0]
        assert "validation" in call_args
        assert call_args["validation"][0]["type"] == "json"
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_json_mode(self, mock_llm):
        """Test ask command with JSON mode."""
        mock_llm.return_value = {"content": '{"result": "structured data"}'}
        
        result = runner.invoke(app, ["ask", "Return JSON", "--json"])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert call_args["response_format"] == {"type": "json_object"}
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_with_system_prompt(self, mock_llm):
        """Test ask command with system prompt."""
        mock_llm.return_value = {"content": "Expert response"}
        
        result = runner.invoke(app, ["ask", "Explain code", "--system", "You are a coding expert"])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert call_args["messages"][0]["role"] == "system"
        assert call_args["messages"][0]["content"] == "You are a coding expert"
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_show_config(self, mock_llm):
        """Test ask command with config display."""
        mock_llm.return_value = {"content": "Response"}
        
        result = runner.invoke(app, ["ask", "Test", "--show-config"])
        
        assert result.exit_code == 0
        assert "LLM Configuration" in result.output
        assert '"messages"' in result.output


class TestChatCommand:
    """Test the 'chat' command functionality."""
    
    @patch('llm_call.cli.main.make_llm_request')
    def test_chat_basic(self, mock_llm):
        """Test basic chat command."""
        mock_llm.return_value = {"content": "Hello! How can I help?"}
        
        # Simulate chat with exit
        result = runner.invoke(app, ["chat"], input="Hello\nexit\n")
        
        assert result.exit_code == 0
        assert "Starting chat session" in result.output
        assert "Ending chat session" in result.output
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_chat_with_system(self, mock_llm):
        """Test chat with system prompt."""
        mock_llm.return_value = {"content": "Code assistance ready"}
        
        result = runner.invoke(app, ["chat", "--system", "You are a coding assistant"], input="exit\n")
        
        assert result.exit_code == 0
        assert "System: You are a coding assistant" in result.output


class TestCallCommand:
    """Test the 'call' command functionality."""
    
    @patch('llm_call.cli.main.make_llm_request')
    def test_call_json_config(self, mock_llm, sample_config_json):
        """Test call command with JSON config."""
        mock_llm.return_value = {"content": "Config response"}
        
        result = runner.invoke(app, ["call", str(sample_config_json)])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert call_args["model"] == "gpt-4"
        assert call_args["temperature"] == 0.7
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_call_yaml_config(self, mock_llm, sample_config_yaml):
        """Test call command with YAML config."""
        mock_llm.return_value = {"content": "YAML response"}
        
        result = runner.invoke(app, ["call", str(sample_config_yaml)])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert call_args["model"] == "gpt-3.5-turbo"
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_call_with_overrides(self, mock_llm, sample_config_json):
        """Test call command with config overrides."""
        mock_llm.return_value = {"content": "Override response"}
        
        result = runner.invoke(app, [
            "call", str(sample_config_json),
            "--prompt", "New prompt",
            "--model", "gpt-3.5-turbo"
        ])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert call_args["model"] == "gpt-3.5-turbo"
        assert call_args["messages"][-1]["content"] == "New prompt"


class TestModelCommands:
    """Test model listing commands."""
    
    def test_models_list_all(self):
        """Test listing all models."""
        result = runner.invoke(app, ["models", "--all"])
        
        assert result.exit_code == 0
        assert "Available Models" in result.output
        assert "OpenAI" in result.output
        assert "gpt-4" in result.output
        assert "Claude CLI" in result.output
        
    def test_models_filter_provider(self):
        """Test filtering models by provider."""
        result = runner.invoke(app, ["models", "--provider", "openai"])
        
        assert result.exit_code == 0
        assert "OpenAI" in result.output
        assert "gpt-4" in result.output
        # Should not show other providers
        assert "Anthropic" not in result.output or "anthropic" not in result.output.lower()


class TestValidatorCommand:
    """Test validator listing command."""
    
    def test_validators_list(self):
        """Test listing validation strategies."""
        result = runner.invoke(app, ["validators"])
        
        assert result.exit_code == 0
        assert "Available Validation Strategies" in result.output


# ============================================
# CONFIGURATION MANAGEMENT TESTS
# ============================================

class TestConfigManagement:
    """Test configuration file handling."""
    
    def test_config_example_json(self, temp_dir):
        """Test generating example JSON config."""
        output_file = temp_dir / "example.json"
        result = runner.invoke(app, ["config-example", "--output", str(output_file)])
        
        assert result.exit_code == 0
        assert output_file.exists()
        
        # Verify it's valid JSON
        config = json.loads(output_file.read_text())
        assert "model" in config
        assert "messages" in config
        
    def test_config_example_yaml(self, temp_dir):
        """Test generating example YAML config."""
        output_file = temp_dir / "example.yaml"
        result = runner.invoke(app, ["config-example", "--output", str(output_file), "--format", "yaml"])
        
        assert result.exit_code == 0
        assert output_file.exists()
        
        # Verify it's valid YAML
        config = yaml.safe_load(output_file.read_text())
        assert "model" in config


# ============================================
# AUTO-GENERATION COMMAND TESTS
# ============================================

class TestGenerateCommands:
    """Test slash command and MCP generation."""
    
    def test_generate_claude(self, temp_dir):
        """Test Claude slash command generation."""
        output_dir = temp_dir / "claude_commands"
        result = runner.invoke(app, ["generate-claude", "--output", str(output_dir)])
        
        assert result.exit_code == 0
        assert "Generated" in result.output
        assert output_dir.exists()
        
        # Check that some command files were created
        json_files = list(output_dir.glob("*.json"))
        assert len(json_files) > 0
        
        # Verify JSON structure
        for json_file in json_files[:1]:  # Check first file
            config = json.loads(json_file.read_text())
            assert "name" in config
            assert "description" in config
            assert "execute" in config
            
    def test_generate_mcp_config(self, temp_dir):
        """Test MCP config generation."""
        output_file = temp_dir / "mcp_config.json"
        result = runner.invoke(app, [
            "generate-mcp-config",
            "--output", str(output_file),
            "--name", "test-cli"
        ])
        
        assert result.exit_code == 0
        assert output_file.exists()
        
        # Verify MCP config structure
        config = json.loads(output_file.read_text())
        assert config["name"] == "test-cli"
        assert "server" in config
        assert "tools" in config
        assert len(config["tools"]) > 0
        
        # Check tool structure
        for tool_name, tool_config in config["tools"].items():
            assert "description" in tool_config
            assert "inputSchema" in tool_config


class TestMCPServer:
    """Test MCP server functionality."""
    
    def test_serve_mcp_no_fastmcp(self):
        """Test serve-mcp command without fastmcp installed."""
        with patch('builtins.__import__', side_effect=ImportError):
            result = runner.invoke(app, ["serve-mcp"])
            
            assert result.exit_code == 1
            assert "FastMCP not installed" in result.output
            
    @patch('llm_call.cli.main.FastMCP')
    def test_serve_mcp_with_fastmcp(self, mock_fastmcp):
        """Test serve-mcp command with fastmcp available."""
        # This is a simplified test since we can't actually start the server
        result = runner.invoke(app, ["serve-mcp", "--debug"])
        
        # Should show registration messages
        assert "Registered" in result.output or "FastMCP" in result.output


# ============================================
# TEST RUNNER COMMAND TESTS  
# ============================================

class TestTestRunnerCommands:
    """Test the test runner functionality."""
    
    def test_test_command_no_files(self, temp_dir):
        """Test 'test' command when no test files exist."""
        result = runner.invoke(app, ["test", "poc_*.py", "--dir", str(temp_dir)])
        
        assert result.exit_code == 0
        assert "No test files found" in result.output
        
    def test_test_command_with_files(self, temp_dir):
        """Test 'test' command with test files."""
        # Create a dummy test file
        test_file = temp_dir / "poc_test.py"
        test_file.write_text("""
print("Running test...")
print("✅ VALIDATION PASSED")
""")
        
        result = runner.invoke(app, ["test", "poc_*.py", "--dir", str(temp_dir)])
        
        assert result.exit_code == 0
        assert "PASSED" in result.output
        assert "Test Summary Report" in result.output
        
    def test_test_poc_list_all(self):
        """Test listing all POC files."""
        result = runner.invoke(app, ["test-poc", "--all"])
        
        # Should exit gracefully even if no POCs found
        assert result.exit_code == 0 or result.exit_code == 1


# ============================================
# README ALIGNMENT TESTS
# ============================================

class TestREADMEAlignment:
    """Verify CLI commands align with README documentation."""
    
    def test_readme_examples_exist(self):
        """Test that commands mentioned in README exist in CLI."""
        # Commands explicitly mentioned in README
        readme_commands = [
            "ask",
            "chat", 
            "call",
            "models",
            "validators",
            "config-example",
            "generate-claude",
            "generate-mcp-config",
            "serve-mcp",
            "test",
            "test-poc"
        ]
        
        # Get actual CLI commands
        cli_commands = [cmd.name or cmd.callback.__name__ for cmd in app.registered_commands]
        
        for cmd in readme_commands:
            assert cmd in cli_commands, f"Command '{cmd}' mentioned in README but not in CLI"
            
    @patch('llm_call.cli.main.make_llm_request')
    def test_readme_ask_examples(self, mock_llm):
        """Test that README 'ask' examples work."""
        mock_llm.return_value = {"content": "Test response"}
        
        # Test examples from README
        examples = [
            ["ask", "What is Python?"],
            ["ask", "Explain AI", "--model", "gpt-4"],
            ["ask", "Write a function", "--validate", "code"],
            ["ask", "Create API", "--model", "gpt-4", "--validate", "code", "--temp", "0.3", "--system", "You are an expert"]
        ]
        
        for example_args in examples:
            result = runner.invoke(app, example_args)
            assert result.exit_code == 0, f"README example failed: {' '.join(example_args)}"
            
    def test_readme_model_routing(self):
        """Test model routing patterns from README."""
        # Test that model patterns mentioned in README are recognized
        readme_models = [
            "anthropic/max",
            "claude/max", 
            "claude-code/max",
            "ollama/llama3.2"
        ]
        
        # This is a basic test - in production you'd test actual routing
        result = runner.invoke(app, ["models", "--all"])
        assert result.exit_code == 0


# ============================================
# INTEGRATION TESTS
# ============================================

class TestIntegration:
    """Test integration between different CLI features."""
    
    @patch('llm_call.cli.main.make_llm_request')
    def test_config_with_validation(self, mock_llm, temp_dir):
        """Test using config file with validation."""
        mock_llm.return_value = {"content": '{"valid": "json"}'}
        
        # Create config with validation
        config = {
            "model": "gpt-4",
            "messages": [{"role": "user", "content": "Generate JSON"}],
            "validation": [{"type": "json"}],
            "response_format": {"type": "json_object"}
        }
        config_file = temp_dir / "validated_config.json"
        config_file.write_text(json.dumps(config))
        
        result = runner.invoke(app, ["call", str(config_file)])
        
        assert result.exit_code == 0
        call_args = mock_llm.call_args[0][0]
        assert "validation" in call_args
        assert call_args["response_format"]["type"] == "json_object"
        
    def test_generate_then_use_commands(self, temp_dir):
        """Test generating commands and verifying they're usable."""
        # Generate Claude commands
        claude_dir = temp_dir / "claude"
        result = runner.invoke(app, ["generate-claude", "--output", str(claude_dir)])
        assert result.exit_code == 0
        
        # Verify 'ask' command was generated
        ask_cmd = claude_dir / "llm-ask.json"
        assert ask_cmd.exists()
        
        config = json.loads(ask_cmd.read_text())
        assert config["name"] == "llm-ask"
        assert "ask" in config["execute"]
        
        # Generate MCP config
        mcp_file = temp_dir / "mcp.json"
        result = runner.invoke(app, ["generate-mcp-config", "--output", str(mcp_file)])
        assert result.exit_code == 0
        
        mcp_config = json.loads(mcp_file.read_text())
        assert "ask" in mcp_config["tools"]


# ============================================
# ERROR HANDLING TESTS
# ============================================

class TestErrorHandling:
    """Test error handling in CLI commands."""
    
    def test_call_missing_config(self):
        """Test call command with missing config file."""
        result = runner.invoke(app, ["call", "nonexistent.json"])
        
        assert result.exit_code == 1
        assert "Error" in result.output
        
    def test_call_invalid_config_format(self, temp_dir):
        """Test call command with unsupported config format."""
        bad_config = temp_dir / "config.txt"
        bad_config.write_text("not json or yaml")
        
        result = runner.invoke(app, ["call", str(bad_config)])
        
        assert result.exit_code == 1
        assert "Error" in result.output
        
    @patch('llm_call.cli.main.make_llm_request')
    def test_ask_llm_failure(self, mock_llm):
        """Test handling LLM request failure."""
        mock_llm.side_effect = Exception("LLM service unavailable")
        
        result = runner.invoke(app, ["ask", "Test"])
        
        assert result.exit_code == 1
        assert "Error" in result.output
        assert "LLM service unavailable" in result.output


# ============================================
# VALIDATION FUNCTION
# ============================================

if __name__ == "__main__":
    """Run validation tests to ensure CLI is working."""
    import subprocess
    
    print("🧪 Running CLI Comprehensive Test Suite")
    print("=" * 60)
    
    # Run pytest with coverage
    result = subprocess.run([
        sys.executable, "-m", "pytest", __file__, 
        "-v", "--tb=short",
        "--cov=llm_call.cli",
        "--cov-report=term-missing"
    ], capture_output=True, text=True)
    
    print(result.stdout)
    if result.stderr:
        print(result.stderr)
        
    if result.returncode == 0:
        print("\n✅ All CLI tests passed!")
        
        # Additional validation
        print("\n📋 Checking CLI help...")
        help_result = subprocess.run([sys.executable, "-m", "llm_call.cli.main", "--help"], 
                                   capture_output=True, text=True)
        
        if help_result.returncode == 0:
            print("✅ CLI help works correctly")
            
            # Count available commands
            import re
            commands = re.findall(r'^\s+(\w+(?:-\w+)*)\s+', help_result.stdout, re.MULTILINE)
            print(f"📊 Found {len(commands)} CLI commands")
            
            # Verify key commands exist
            key_commands = ["ask", "chat", "call", "models", "test", "generate-claude"]
            missing = [cmd for cmd in key_commands if cmd not in commands]
            
            if missing:
                print(f"❌ Missing commands: {missing}")
            else:
                print("✅ All key commands present")
        else:
            print("❌ CLI help failed")
            
    else:
        print("\n❌ Some tests failed!")
        sys.exit(1)