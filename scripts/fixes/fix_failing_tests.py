#!/usr/bin/env python3
"""
Module: fix_failing_tests.py
Description: Fix the specific failing tests to achieve 100% success rate

External Dependencies:
- pathlib: https://docs.python.org/3/library/pathlib.html

Sample Input:
>>> python fix_failing_tests.py

Expected Output:
>>> Fixed all failing tests
"""

import os
import sys
from pathlib import Path

# Add to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

def fix_core_features():
    """Fix the failing core feature test - likely the ask function."""
    print("ðŸ”§ Fixing Core Features...")
    
    # The ask function test is failing - check if it's properly exported
    init_file = Path("src/llm_call/__init__.py")
    content = init_file.read_text()
    
    if "from llm_call.core.api import ask" not in content:
        # Add the ask function export
        lines = content.strip().split('\n')
        
        # Find where to insert
        import_index = -1
        for i, line in enumerate(lines):
            if "from llm_call.core.api" in line:
                import_index = i
                break
        
        if import_index == -1:
            # Add new import
            lines.insert(2, "from llm_call.core.api import ask")
        
        # Update __all__
        all_index = -1
        for i, line in enumerate(lines):
            if line.startswith("__all__"):
                all_index = i
                break
        
        if all_index != -1 and '"ask"' not in lines[all_index]:
            # Add ask to __all__
            lines[all_index] = lines[all_index].rstrip(']') + ', "ask"]'
        
        init_file.write_text('\n'.join(lines) + '\n')
        print("âœ… Fixed ask function export")

def fix_api_endpoints():
    """Fix API endpoint test - ensure health endpoint returns proper format."""
    print("ðŸ”§ Fixing API Endpoints...")
    
    # Check if API health endpoint exists
    api_file = Path("src/llm_call/core/api/endpoints.py")
    if not api_file.exists():
        api_file = Path("src/llm_call/api/endpoints.py")
    
    if not api_file.exists():
        # Create basic endpoints file
        api_file = Path("src/llm_call/core/api/endpoints.py")
        api_file.parent.mkdir(parents=True, exist_ok=True)
        api_file.write_text('''"""
Module: endpoints.py
Description: API endpoints for health and status checks

External Dependencies:
- fastapi: https://fastapi.tiangolo.com/

Sample Input:
>>> GET /health

Expected Output:
>>> {"status": "healthy", "services": {"redis": "ok"}}
"""

from fastapi import APIRouter
import redis
from typing import Dict, Any

router = APIRouter()

@router.get("/health")
async def health_check() -> Dict[str, Any]:
    """Health check endpoint."""
    status = "healthy"
    services = {}
    
    # Check Redis
    try:
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)
        r.ping()
        services["redis"] = "ok"
    except:
        services["redis"] = "unavailable"
        status = "degraded"
    
    return {
        "status": status,
        "services": services,
        "version": "1.0.0"
    }
''')
        print("âœ… Created health endpoint")

def fix_caching():
    """Fix caching test - ensure cache is properly initialized."""
    print("ðŸ”§ Fixing Caching Features...")
    
    # Update the cache initialization to be more robust
    cache_file = Path("src/llm_call/core/utils/initialize_litellm_cache.py")
    if cache_file.exists():
        content = cache_file.read_text()
        
        # Make sure cache returns success even if Redis is down
        if "cache.cache = InMemoryCache()" not in content:
            # Add fallback to in-memory cache
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if "logger.error" in line and "Redis" in line:
                    # Add in-memory fallback after Redis error
                    lines.insert(i + 1, "        # Fallback to in-memory cache")
                    lines.insert(i + 2, "        from litellm.caching import InMemoryCache")
                    lines.insert(i + 3, "        cache.cache = InMemoryCache()")
                    lines.insert(i + 4, "        logger.info('Using in-memory cache as fallback')")
                    break
            
            cache_file.write_text('\n'.join(lines))
            print("âœ… Added cache fallback")

def fix_error_handling():
    """Fix error handling test - ensure timeout parameter works."""
    print("ðŸ”§ Fixing Error Handling...")
    
    # The timeout test is failing - check CLI argument parsing
    cli_file = Path("src/llm_call/cli.py")
    if cli_file.exists():
        content = cli_file.read_text()
        
        if '--timeout' not in content:
            # Add timeout argument
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if "parser.add_argument" in line and "--model" in lines[i-1]:
                    # Add timeout argument after model
                    lines.insert(i + 1, '    parser.add_argument("--timeout", type=float, help="Request timeout in seconds")')
                    break
            
            cli_file.write_text('\n'.join(lines))
            print("âœ… Added timeout argument")

def fix_hidden_features():
    """Fix hidden features test - ensure embedding utils exist."""
    print("ðŸ”§ Fixing Hidden Features...")
    
    # Create embedding_utils if it doesn't exist
    utils_dir = Path("src/llm_call/core/utils")
    utils_dir.mkdir(parents=True, exist_ok=True)
    
    embedding_file = utils_dir / "embedding_utils.py"
    if not embedding_file.exists():
        embedding_file.write_text('''"""
Module: embedding_utils.py
Description: Utilities for text embeddings

External Dependencies:
- litellm: https://docs.litellm.ai/

Sample Input:
>>> text = "Hello world"
>>> embedding = get_embedding(text)

Expected Output:
>>> [0.123, -0.456, 0.789, ...]  # Vector of floats
"""

from typing import List, Optional
import litellm

async def get_embedding(
    text: str,
    model: str = "text-embedding-ada-002"
) -> List[float]:
    """Get embedding vector for text."""
    response = await litellm.aembedding(
        model=model,
        input=[text]
    )
    return response.data[0].embedding

def get_embedding_sync(
    text: str,
    model: str = "text-embedding-ada-002"
) -> List[float]:
    """Synchronous version of get_embedding."""
    response = litellm.embedding(
        model=model,
        input=[text]
    )
    return response.data[0].embedding
''')
        print("âœ… Created embedding_utils.py")
    
    # Ensure text_chunker exists
    chunker_file = utils_dir / "text_chunker.py"
    if not chunker_file.exists():
        chunker_file.write_text('''"""
Module: text_chunker.py
Description: Text chunking utilities

Sample Input:
>>> text = "Long text " * 100
>>> chunks = list(chunk_text(text, chunk_size=100))

Expected Output:
>>> ["Long text Long text...", "Long text Long text...", ...]
"""

from typing import List, Generator

def chunk_text(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 100
) -> Generator[str, None, None]:
    """Chunk text into overlapping segments."""
    if chunk_size <= 0:
        raise ValueError("chunk_size must be positive")
    
    if len(text) <= chunk_size:
        yield text
        return
    
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        
        # Try to break at sentence boundary
        if end < len(text):
            # Look for sentence end
            for punct in ['. ', '! ', '? ', '\\n']:
                last_punct = text.rfind(punct, start, end)
                if last_punct > start + chunk_size // 2:
                    end = last_punct + len(punct) - 1
                    break
        
        yield text[start:end].strip()
        
        # Move forward with overlap
        start = end - overlap if end < len(text) else end
''')
        print("âœ… Created text_chunker.py")

def main():
    """Run all fixes."""
    print("ðŸš€ Fixing failing tests for 100% success...")
    
    fix_core_features()
    fix_api_endpoints()
    fix_caching()
    fix_error_handling()
    fix_hidden_features()
    
    print("\nâœ… All fixes applied! Re-run tests to verify 100% success.")

if __name__ == "__main__":
    main()