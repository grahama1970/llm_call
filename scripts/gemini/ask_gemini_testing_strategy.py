#!/usr/bin/env python3
"""
Script to ask Gemini Flash 2.0 about testing strategy for Claude Code projects.

This script uses the LLM Call interface to ask Gemini about best practices
for testing when Claude Code cannot accurately report test results.
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from loguru import logger

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from llm_call.core.caller import make_llm_request


async def ask_gemini_about_testing():
    """Ask Gemini Flash 2.0 about testing strategy."""
    
    question = """I'm working with Claude Code on a Python project (llm_call - a universal LLM interface). Claude Code has a fundamental limitation where it cannot accurately report test results - it consistently claims tests pass when they fail.

Given this limitation:
1. Should we convert pytest tests to usage functions with inline assertions (if __name__ == '__main__' blocks)?
2. Would usage functions with explicit print statements and sys.exit(1) on failure be more reliable?
3. What's the best practice for debugging/testing when the AI assistant cannot be trusted to report results?

The project is straightforward - routing LLM calls to different providers (OpenAI, Anthropic, Google). We need a reliable way to verify functionality."""
    
    config = {
        "model": "gemini-2.0-flash-exp",  # Using Gemini Flash 2.0
        "messages": [
            {
                "role": "user",
                "content": question
            }
        ],
        "temperature": 0.7,
        "max_tokens": 2000
    }
    
    logger.info("Asking Gemini Flash 2.0 about testing strategy...")
    
    try:
        response = await make_llm_request(config)
        
        if response:
            # Extract content based on response type
            if hasattr(response, 'choices'):
                # LiteLLM ModelResponse
                content = response.choices[0].message.content
            elif isinstance(response, dict):
                # Direct dict response
                content = response.get('content', str(response))
            else:
                content = str(response)
            
            # Save response to file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = Path(f"docs/reports/gemini_testing_strategy_{timestamp}.md")
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            report = f"""# Gemini Flash 2.0 Testing Strategy Advice

**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Model:** gemini-2.0-flash-exp

## Question

{question}

## Gemini's Response

{content}

## Analysis Summary

Based on Gemini's advice, here are the key takeaways:

1. **Usage Functions vs Pytest**: {
    "Convert to usage functions" if "usage function" in content.lower() and "yes" in content.lower()[:200] 
    else "Keep pytest but verify externally"
}

2. **Explicit Output**: {
    "Recommended" if "print" in content.lower() and "explicit" in content.lower() 
    else "May help but not sufficient alone"
}

3. **Best Practices**: Look for external verification methods mentioned in the response above.

---
Generated by ask_gemini_testing_strategy.py
"""
            
            output_file.write_text(report)
            logger.success(f"Response saved to: {output_file}")
            
            # Also print key points to console
            print("\n" + "="*60)
            print("GEMINI'S TESTING STRATEGY ADVICE")
            print("="*60)
            print(content)
            print("="*60)
            
            return content
            
    except Exception as e:
        logger.error(f"Failed to get response from Gemini: {e}")
        return None


async def analyze_response(response_content):
    """Analyze Gemini's response for actionable recommendations."""
    
    if not response_content:
        return
    
    # Ask Gemini to summarize its own advice into actionable steps
    config = {
        "model": "gemini-2.0-flash-exp",
        "messages": [
            {
                "role": "user",
                "content": f"""Based on this advice about testing with Claude Code:

{response_content}

Please provide a concise ACTION PLAN with:
1. Immediate steps to implement (numbered list)
2. Example code structure for a usage function
3. Verification checklist

Keep it practical and focused on the llm_call project."""
            }
        ],
        "temperature": 0.3,  # Lower temperature for more focused response
        "max_tokens": 1000
    }
    
    logger.info("Getting actionable summary from Gemini...")
    
    try:
        response = await make_llm_request(config)
        
        if response:
            # Extract content
            if hasattr(response, 'choices'):
                content = response.choices[0].message.content
            elif isinstance(response, dict):
                content = response.get('content', str(response))
            else:
                content = str(response)
            
            print("\n" + "="*60)
            print("ACTIONABLE SUMMARY")
            print("="*60)
            print(content)
            print("="*60)
            
            # Save action plan
            action_file = Path("docs/reports/gemini_testing_action_plan.md")
            action_file.write_text(f"""# Gemini's Testing Action Plan for llm_call

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

{content}
""")
            logger.success(f"Action plan saved to: {action_file}")
            
    except Exception as e:
        logger.error(f"Failed to get action plan: {e}")


async def main():
    """Main function."""
    
    # First, ask the main question
    response = await ask_gemini_about_testing()
    
    if response:
        # Then get actionable summary
        await analyze_response(response)
        logger.success("Testing strategy consultation complete!")
    else:
        logger.error("Failed to get testing strategy advice from Gemini")
        sys.exit(1)


if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())