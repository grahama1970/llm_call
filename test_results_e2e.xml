<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="1" failures="1" skipped="2" tests="11" time="5.224" timestamp="2025-06-10T09:44:41.916321-04:00" hostname="graham-MS-7C60"><testcase classname="tests.e2e.test_hello_world_simple" name="test_model" time="0.000"><error message="failed on setup with &quot;file /home/graham/workspace/experiments/llm_call/tests/e2e/test_hello_world_simple.py, line 47&#10;  async def test_model(model: str) -&gt; Dict[str, Any]:&#10;      &quot;&quot;&quot;Test a single model with Hello World prompt.&quot;&quot;&quot;&#10;      result = {&#10;          &quot;model&quot;: model,&#10;          &quot;status&quot;: &quot;unknown&quot;,&#10;          &quot;response&quot;: None,&#10;          &quot;error&quot;: None,&#10;          &quot;duration&quot;: 0&#10;      }&#10;&#10;      start_time = asyncio.get_event_loop().time()&#10;&#10;      try:&#10;          response = await make_llm_request({&#10;              &quot;model&quot;: model,&#10;              &quot;messages&quot;: [&#10;                  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say exactly: Hello World!&quot;}&#10;              ],&#10;              &quot;temperature&quot;: 0.1,&#10;              &quot;max_tokens&quot;: 100&#10;          })&#10;&#10;          duration = asyncio.get_event_loop().time() - start_time&#10;&#10;          if response:&#10;              # Extract content based on response type&#10;              if hasattr(response, 'choices') and response.choices:&#10;                  content = response.choices[0].message.content&#10;              elif isinstance(response, dict):&#10;                  content = response.get('content', response.get('response', ''))&#10;              else:&#10;                  content = str(response)&#10;&#10;              result[&quot;status&quot;] = &quot;success&quot;&#10;              result[&quot;response&quot;] = content&#10;              result[&quot;duration&quot;] = duration&#10;          else:&#10;              result[&quot;status&quot;] = &quot;no_response&quot;&#10;              result[&quot;error&quot;] = &quot;No response received&quot;&#10;&#10;      except Exception as e:&#10;          result[&quot;status&quot;] = &quot;error&quot;&#10;          result[&quot;error&quot;] = str(e)&#10;          result[&quot;duration&quot;] = asyncio.get_event_loop().time() - start_time&#10;&#10;      return result&#10;E       fixture 'model' not found&#10;&gt;       available fixtures: __pytest_repeat_step_number, _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, extended_user_prompts, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, test_data_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_prompts, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/home/graham/workspace/experiments/llm_call/tests/e2e/test_hello_world_simple.py:47&quot;">file /home/graham/workspace/experiments/llm_call/tests/e2e/test_hello_world_simple.py, line 47
  async def test_model(model: str) -&gt; Dict[str, Any]:
      """Test a single model with Hello World prompt."""
      result = {
          "model": model,
          "status": "unknown",
          "response": None,
          "error": None,
          "duration": 0
      }

      start_time = asyncio.get_event_loop().time()

      try:
          response = await make_llm_request({
              "model": model,
              "messages": [
                  {"role": "user", "content": "Say exactly: Hello World!"}
              ],
              "temperature": 0.1,
              "max_tokens": 100
          })

          duration = asyncio.get_event_loop().time() - start_time

          if response:
              # Extract content based on response type
              if hasattr(response, 'choices') and response.choices:
                  content = response.choices[0].message.content
              elif isinstance(response, dict):
                  content = response.get('content', response.get('response', ''))
              else:
                  content = str(response)

              result["status"] = "success"
              result["response"] = content
              result["duration"] = duration
          else:
              result["status"] = "no_response"
              result["error"] = "No response received"

      except Exception as e:
          result["status"] = "error"
          result["error"] = str(e)
          result["duration"] = asyncio.get_event_loop().time() - start_time

      return result
E       fixture 'model' not found
&gt;       available fixtures: __pytest_repeat_step_number, _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, extended_user_prompts, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, test_data_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_prompts, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/home/graham/workspace/experiments/llm_call/tests/e2e/test_hello_world_simple.py:47</error></testcase><testcase classname="tests.e2e.test_llm_call" name="test_basic_import" time="0.001" /><testcase classname="tests.e2e.test_llm_call" name="test_model_routing" time="0.001" /><testcase classname="tests.e2e.test_llm_call" name="test_simple_call" time="0.764" /><testcase classname="tests.e2e.test_llm_call" name="test_proxy_config" time="0.012" /><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_vertex_ai_hello_world" time="1.071" /><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_gemini_direct_hello_world" time="0.684" /><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_openai_hello_world" time="0.443" /><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_claude_opus_hello_world" time="2.004"><skipped type="pytest.skip" message="Claude proxy not accessible on port 3010">/home/graham/workspace/experiments/llm_call/tests/e2e/test_model_hello_world.py:246: Claude proxy not accessible on port 3010</skipped></testcase><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_claude_direct_hello_world" time="0.173"><failure message="AssertionError: No response from claude-3-sonnet-20240229&#10;assert None is not None">tests/e2e/test_model_hello_world.py:312: in test_claude_direct_hello_world
    result = self._verify_response(response, model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/e2e/test_model_hello_world.py:70: in _verify_response
    assert response is not None, f"No response from {model}"
E   AssertionError: No response from claude-3-sonnet-20240229
E   assert None is not None</failure></testcase><testcase classname="tests.e2e.test_model_hello_world.TestModelHelloWorld" name="test_model_comparison" time="0.000"><skipped type="pytest.skip" message="No test results to compare">/home/graham/workspace/experiments/llm_call/tests/e2e/test_model_hello_world.py:334: No test results to compare</skipped></testcase></testsuite></testsuites>